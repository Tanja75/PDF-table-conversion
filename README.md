# PDF-table-conversion
Copying PDF table to Excel usually doesn't work. Especially when trying to get publically available data on the internet. Web converters can help to some extent, but usually there is plenty job to do to fix all the data in a manner that can be used further, and there's an issue of data credibility.
With this project I tried to use Python's module Tabula to convert table from pdf to scv file. Actually, at first, I tried to convert it just with public interfaces from tabula - read_pdf (df = tabula.read_pdf("/path/to/sample.pdf", pages="all")) but it also created a mess. I figured out that the table was actually consisting of two parts, and that is how I came up with an idea to import two tables separately and merge them after that, and it worked. After some minor rearrangements such as merging first two columns to get completely defined feature (parameter along with measuring units) and rearranging the order of columns, and changing all NaN values to 0 (since in analytical chemistry there is no 0 value, there is only value that is below the detection limit, but in order to be able to present the values graphically, in this case, I marked them as 0, with the mark that 0 is to be interpreted as "below the detection limit") I came with the shape that can be transposed in order to get data in more meaningful way to see some trends, namely, to get features (in this case different parameters monitored) as columns and observations (in this case dates) as rows. The biggest problem that I encountered is the fact that data were imported as objects. And after trying to convert the data in int and float I have lost all the data that had "," as decimal separator. Also, when trying to use decimal="," as pd.read_scv attribute, nothing happened.....So that is the problem that I still have to find solution for. Although, I managed to get visual representation of features  that contained only int values.
